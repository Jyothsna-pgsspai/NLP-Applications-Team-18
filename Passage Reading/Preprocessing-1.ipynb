{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Preprocessing-1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvsrf/zGuBkvy0jx4eFy25"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"WvxKr_JpJw9T","colab_type":"code","outputId":"6af04233-5005-4acf-c51c-c59abb5433cc","executionInfo":{"status":"ok","timestamp":1587178858670,"user_tz":-330,"elapsed":55863,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TQFHiV2hJ15_","colab_type":"code","outputId":"8195e9cc-7e81-42cd-b7a0-6549dd3d83cf","executionInfo":{"status":"ok","timestamp":1587178885008,"user_tz":-330,"elapsed":2372,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd \"/content/gdrive/My Drive/Project/drqa/scripts\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Project/drqa/scripts\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XUq5eaJShzxX","colab_type":"code","outputId":"44b677b2-32f6-4036-b3f8-9767440af4f5","executionInfo":{"status":"ok","timestamp":1587178929679,"user_tz":-330,"elapsed":3399,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["convert  drqa  Untitled0.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-L_uw3BKh0uN","colab_type":"code","colab":{}},"source":["import re\n","import json\n","import spacy\n","import msgpack\n","import unicodedata\n","import numpy as np\n","import argparse\n","import collections\n","import multiprocessing\n","from multiprocessing import Pool\n","from tqdm import tqdm\n","from functools import partial\n","from drqa.utils import str2bool\n","import logging\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P--wx3DNilpU","colab_type":"code","colab":{}},"source":["#Creating a class of arguments\n","class arguments:\n","  trn_file='/content/gdrive/My Drive/Project/drqa/datasets/squad/train-v1.1.json'\n","  dev_file='/content/gdrive/My Drive/Project/drqa/datasets/squad/dev-v1.1.json'\n","  wv_file='/content/gdrive/My Drive/Project/drqa/datasets/glove.840B.300d.txt'\n","  wv_dim=300\n","  wv_cased=True\n","  sort_all=True\n","  sample_size=0\n","  threads=2\n","  batch_size=64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hz_XB7iQwYM","colab_type":"code","colab":{}},"source":["args=arguments()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjcxXS_NSvwi","colab_type":"code","outputId":"d533b127-e00f-45b4-993e-1beb33204f24","executionInfo":{"status":"ok","timestamp":1587179963446,"user_tz":-330,"elapsed":1557,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG,\n","                        datefmt='%m/%d/%Y %I:%M:%S')\n","log = logging.getLogger(__name__)\n","log.info(vars(args))\n","log.info('start data preparing...')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 03:19:21 {}\n","04/18/2020 03:19:21 start data preparing...\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YcnP1SOIg-dQ","colab_type":"text"},"source":["Considering only context, question and answer in paragraph in training data. But in dev data there are os may answers wheras in training only one answer.\n"]},{"cell_type":"code","metadata":{"id":"QKQA3szBTG89","colab_type":"code","colab":{}},"source":["def flatten_json(data_file, mode):\n","    \"\"\"Flatten each article in training data.\"\"\"\n","    with open(data_file) as f:\n","        data = json.load(f)['data']\n","    rows = []\n","    for article in data:\n","        for paragraph in article['paragraphs']:\n","            context = paragraph['context']\n","            for qa in paragraph['qas']:\n","                id_, question, answers = qa['id'], qa['question'], qa['answers']\n","                if mode == 'train':\n","                    answer = answers[0]['text']  # in training data there's only one answer\n","                    answer_start = answers[0]['answer_start']\n","                    answer_end = answer_start + len(answer)\n","                    rows.append((id_, context, question, answer, answer_start, answer_end))\n","                else:  # mode == 'dev'\n","                    answers = [a['text'] for a in answers]\n","                    rows.append((id_, context, question, answers))\n","    return rows"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqpxnLy3TRnN","colab_type":"code","outputId":"42e98f92-0717-41a1-e999-435711eb5659","executionInfo":{"status":"ok","timestamp":1587180275904,"user_tz":-330,"elapsed":2940,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train = flatten_json(args.trn_file, 'train')\n","dev = flatten_json(args.dev_file, 'dev')\n","log.info('json data flattened.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 03:24:34 json data flattened.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wX2ko04LV3om","colab_type":"code","colab":{}},"source":["def clean_spaces(text):\n","    \"\"\"normalize spaces in a string.\"\"\"\n","    text = re.sub(r'\\s', ' ', text)\n","    return text\n","\n","\n","def normalize_text(text):\n","    return unicodedata.normalize('NFD', text) #Decompose if any greek type words are present in text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFeRZNDOV5is","colab_type":"code","colab":{}},"source":["nlp=None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovJ6gF0WV9tB","colab_type":"code","colab":{}},"source":["def init():\n","    \"\"\"initialize spacy in each process\"\"\"\n","    global nlp\n","    nlp = spacy.load('en', parser=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaNHeC9vWBg_","colab_type":"code","colab":{}},"source":["def annotate(row, wv_cased):\n","    global nlp\n","    id_, context, question = row[:3]\n","    q_doc = nlp(clean_spaces(question)) #Clean any unnecessary spaces in question\n","    c_doc = nlp(clean_spaces(context)) #Clean any unnecessary spaces in context\n","    question_tokens = [normalize_text(w.text) for w in q_doc] #Normalize question tokens\n","    context_tokens = [normalize_text(w.text) for w in c_doc]  #Normalize context tokens\n","    question_tokens_lower = [w.lower() for w in question_tokens] #Lower question tokens\n","    context_tokens_lower = [w.lower() for w in context_tokens] #Lower context tokens\n","    context_token_span = [(w.idx, w.idx + len(w.text)) for w in c_doc] #Set the pair for span of the word in context words \n","    context_tags = [w.tag_ for w in c_doc] #POS tags for context words Eg: NN, VBZ, NNP\n","    context_ents = [w.ent_type_ for w in c_doc] #NER types for context words Eg:NORP, Work-of-art\n","    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in q_doc} #Apply Lemmaization for question words\n","    question_tokens_set = set(question_tokens) \n","    question_tokens_lower_set = set(question_tokens_lower)\n","    match_origin = [w in question_tokens_set for w in context_tokens] #Check if query word is present in context\n","    match_lower = [w in question_tokens_lower_set for w in context_tokens_lower] #Check if query lower words are present in context lower words\n","    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in c_doc] #apply similarity check for matching lemma for question words and context words\n","    # term frequency in document\n","    counter_ = collections.Counter(context_tokens_lower) #number of times each word is repeating in ocntext\n","    total = len(context_tokens_lower) #Total number of words in context\n","    context_tf = [counter_[w] / total for w in context_tokens_lower] #Each word in context frequency probability \n","    context_features = list(zip(match_origin, match_lower, match_lemma, context_tf)) #Zip all the features match_origin, match_lower, match_lemma, probability\n","    if not wv_cased:\n","        context_tokens = context_tokens_lower\n","        question_tokens = question_tokens_lower\n","    return (id_, context_tokens, context_features, context_tags, context_ents,\n","            question_tokens, context, context_token_span) + row[3:]  #Adding all these alogn with answer,answer_start,answer_end"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mbG1xH9Uh5Ci","colab_type":"text"},"source":["Building annotations to training and dev data."]},{"cell_type":"code","metadata":{"id":"OEHRslo7WfXh","colab_type":"code","outputId":"a2174a4c-e775-4a86-eab7-d59a0ac4f520","executionInfo":{"status":"ok","timestamp":1587183641954,"user_tz":-330,"elapsed":2757246,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["with Pool(args.threads, initializer=init) as p:\n","        annotate_ = partial(annotate, wv_cased=args.wv_cased)\n","        train = list(tqdm(p.imap(annotate_, train, chunksize=args.batch_size), total=len(train), desc='train'))\n","        dev = list(tqdm(p.imap(annotate_, dev, chunksize=args.batch_size), total=len(dev), desc='dev  '))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train: 100%|██████████| 87599/87599 [40:53<00:00, 35.70it/s]\n","dev  : 100%|██████████| 10570/10570 [05:02<00:00, 34.98it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MmsKTrTtWpZf","colab_type":"code","colab":{}},"source":["def index_answer(row):\n","    token_span = row[-4] #Context tokens span\n","    starts, ends = zip(*token_span) #starting index of token is stored in starts and ending index of span is stored in ends\n","    answer_start = row[-2]\n","    answer_end = row[-1]\n","    try:\n","        return row[:-3] + (starts.index(answer_start), ends.index(answer_end)) #train till context token spans is added and index of answer start in starts and index of anser end in ends\n","    except ValueError:\n","        return row[:-3] + (None, None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAB0JB1ch0XU","colab_type":"code","outputId":"8df63316-5daf-4e6f-e0b6-2bf8aa7cd90b","executionInfo":{"status":"ok","timestamp":1587183869093,"user_tz":-330,"elapsed":2331,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train = list(map(index_answer, train))\n","initial_len = len(train)\n","train = list(filter(lambda x: x[-1] is not None, train)) #Removing any empty features like answer is not present in starts and ends\n","log.info('drop {} inconsistent samples.'.format(initial_len - len(train)))\n","log.info('tokens generated')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 04:24:27 drop 999 inconsistent samples.\n","04/18/2020 04:24:27 tokens generated\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OZmafNyeh-Z-","colab_type":"code","outputId":"39126688-102e-4d0d-f26c-bb515bfb5943","executionInfo":{"status":"ok","timestamp":1587186174996,"user_tz":-330,"elapsed":83730,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":[" # load vocabulary from word vector files\n","wv_vocab = set()\n","with open(args.wv_file) as f:\n","        for line in f:\n","            token = normalize_text(line.rstrip().split(' ')[0])\n","            wv_vocab.add(token)\n","log.info('glove vocab loaded.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 05:02:52 glove vocab loaded.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Aaey763cqde7","colab_type":"code","colab":{}},"source":["def build_vocab(questions, contexts, wv_vocab, sort_all=False):\n","    \"\"\"\n","    Build vocabulary sorted by global word frequency, or consider frequencies in questions first,\n","    which is controlled by `args.sort_all`.\n","    \"\"\"\n","    if sort_all: \n","        counter = collections.Counter(w for doc in questions + contexts for w in doc) #number of times words in question appeared and words in context appeared and their together count.\n","        vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True) #Those question words present in glove vocab are stored as vocab\n","    else:\n","        counter_q = collections.Counter(w for doc in questions for w in doc)\n","        counter_c = collections.Counter(w for doc in contexts for w in doc)\n","        counter = counter_c + counter_q\n","        vocab = sorted([t for t in counter_q if t in wv_vocab], key=counter_q.get, reverse=True)\n","        vocab += sorted([t for t in counter_c.keys() - counter_q.keys() if t in wv_vocab],\n","                        key=counter.get, reverse=True)\n","    vocab.insert(0, \"<PAD>\")\n","    vocab.insert(1, \"<UNK>\")\n","    return vocab, counter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1sRNSq1q7HS","colab_type":"code","outputId":"49240061-60cf-4430-991f-3a875dc02d0e","executionInfo":{"status":"ok","timestamp":1587186338447,"user_tz":-330,"elapsed":10965,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":[" # build vocabulary\n","    full = train + dev\n","    #row[5] means question_tokens and row[1] means context_tokens\n","    vocab, counter = build_vocab([row[5] for row in full], [row[1] for row in full], wv_vocab, args.sort_all)\n","    total = sum(counter.values()) #Total occurences of question words\n","    matched = sum(counter[t] for t in vocab) #Total occurences of words in vocab\n","    log.info('vocab coverage {1}/{0} | OOV occurrence {2}/{3} ({4:.4f}%)'.format(\n","        len(counter), len(vocab), (total - matched), total, (total - matched) / total * 100))\n","    #Here row[3] means context POS tags\n","    counter_tag = collections.Counter(w for row in full for w in row[3]) #how many times each pos tag occured along with their frequencies\n","    vocab_tag = sorted(counter_tag, key=counter_tag.get, reverse=True) #Sorting of pos tags based on their frequency\n","    #Here row[4] means context NER tags\n","    counter_ent = collections.Counter(w for row in full for w in row[4]) #how many times each NER type occured along with their frequencies\n","    vocab_ent = sorted(counter_ent, key=counter_ent.get, reverse=True) #sorting of NER tags based on their frequencies\n","    #enumerate all words in vocab (<PAD>,<UNK>), vocab_tags and vocab_ent \n","    w2id = {w: i for i, w in enumerate(vocab)}\n","    tag2id = {w: i for i, w in enumerate(vocab_tag)}\n","    ent2id = {w: i for i, w in enumerate(vocab_ent)}\n","    log.info('Vocabulary size: {}'.format(len(vocab)))\n","    log.info('Found {} POS tags.'.format(len(vocab_tag)))\n","    log.info('Found {} entity tags: {}'.format(len(vocab_ent), vocab_ent))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 05:05:32 vocab coverage 91590/111017 | OOV occurrence 110079/14779333 (0.7448%)\n","04/18/2020 05:05:36 Vocabulary size: 91590\n","04/18/2020 05:05:36 Found 50 POS tags.\n","04/18/2020 05:05:36 Found 19 entity tags: ['', 'ORG', 'DATE', 'PERSON', 'GPE', 'NORP', 'CARDINAL', 'LOC', 'FAC', 'WORK_OF_ART', 'EVENT', 'PERCENT', 'QUANTITY', 'ORDINAL', 'MONEY', 'LAW', 'PRODUCT', 'TIME', 'LANGUAGE']\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qEUpvMBrXjeL","colab_type":"text"},"source":["row\n","0:id_,1 :context_tokens, 2:context_features, 3:context_tags, 4:context_ents, 5:question_tokens,6: context,7: context_token_span,8: starts.index(answer_start.index),9:ends.index(sanswers_end.index)"]},{"cell_type":"code","metadata":{"id":"JQfr0htzrWqA","colab_type":"code","colab":{}},"source":["def to_id(row, w2id, tag2id, ent2id, unk_id=1):\n","    context_tokens = row[1]\n","    context_features = row[2]\n","    context_tags = row[3]\n","    context_ents = row[4]\n","    question_tokens = row[5]\n","    question_ids = [w2id[w] if w in w2id else unk_id for w in question_tokens]\n","    context_ids = [w2id[w] if w in w2id else unk_id for w in context_tokens]\n","    tag_ids = [tag2id[w] for w in context_tags]\n","    ent_ids = [ent2id[w] for w in context_ents]\n","    return (row[0], context_ids, context_features, tag_ids, ent_ids, question_ids) + row[6:]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuMCWbxBDhRb","colab_type":"text"},"source":["row[0] --> id_, context_ids, context_features, tag_ids, ent_ids, question_ids, row[6:] --> context, context_token_span, starts.index(answer_start.index),9:ends.index(sanswers_end.index)\n","\n"]},{"cell_type":"code","metadata":{"id":"QQzh8K_Vrkiz","colab_type":"code","outputId":"b2d057dd-18f1-42b0-d0bf-e2c12b217a93","executionInfo":{"status":"ok","timestamp":1587186429048,"user_tz":-330,"elapsed":11472,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["to_id_ = partial(to_id, w2id=w2id, tag2id=tag2id, ent2id=ent2id)\n","train = list(map(to_id_, train))\n","dev = list(map(to_id_, dev))\n","log.info('converted to ids.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 05:07:07 converted to ids.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-apJSElmrshH","colab_type":"code","colab":{}},"source":["vocab_size = len(vocab) #178\n","embeddings = np.zeros((vocab_size, args.wv_dim)) #(178,300)\n","embed_counts = np.zeros(vocab_size) #(178,)\n","embed_counts[:2] = 1  # PADDING & UNK #(178,)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkLfreE4r0Vl","colab_type":"code","outputId":"5f858eb9-efe7-4a25-fdb0-50a433f67cf0","executionInfo":{"status":"ok","timestamp":1587186576308,"user_tz":-330,"elapsed":106854,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open(args.wv_file) as f:\n","        for line in f:\n","            elems = line.rstrip().split(' ')\n","            token = normalize_text(elems[0])\n","            if token in w2id:\n","                word_id = w2id[token]\n","                embed_counts[word_id] += 1\n","                embeddings[word_id] += [float(v) for v in elems[1:]]\n","embeddings /= embed_counts.reshape((-1, 1))\n","log.info('got embedding matrix.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 05:09:34 got embedding matrix.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6NXxnsLvr5EY","colab_type":"code","colab":{}},"source":["meta = {\n","        'vocab': vocab, #All Vocabulary\n","        'vocab_tag': vocab_tag, #All vocab POS tags\n","        'vocab_ent': vocab_ent, #All vocab NER tags\n","        'embedding': embeddings.tolist(), #thier respective embeddings\n","        'wv_cased': args.wv_cased,\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZtNNAnor9C4","colab_type":"code","colab":{}},"source":["# train: id, context_id, context_features, tag_id, ent_id,\n","#        question_id, context, context_token_span, answer_start, answer_end\n","# dev:   id, context_id, context_features, tag_id, ent_id,\n","#        question_id, context, context_token_span, answer\n","with open('meta.msgpack', 'wb') as f:\n","        msgpack.dump(meta, f)\n","result = {\n","        'train': train,\n","        'dev': dev\n","        }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"86TOyspdsd5O","colab_type":"code","colab":{}},"source":["with open('data.msgpack', 'wb') as f:\n","        msgpack.dump(result, f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTdgoq10kq63","colab_type":"text"},"source":["Sample size is defined for any debuggin gprocess. Here, we are not assigning smaplesize."]},{"cell_type":"code","metadata":{"id":"tM-5In03siYC","colab_type":"code","outputId":"b209fd23-d077-4555-f1b8-eec1fb957f5c","executionInfo":{"status":"ok","timestamp":1587186690580,"user_tz":-330,"elapsed":1189,"user":{"displayName":"Jyothsna Munipalle","photoUrl":"","userId":"01874070639261093213"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["if args.sample_size:\n","        sample = {\n","            'train': train[:args.sample_size],\n","            'dev': dev[:args.sample_size]\n","        }\n","        with open('SQuAD/sample.msgpack', 'wb') as f:\n","            msgpack.dump(sample, f)\n","log.info('saved to disk.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["04/18/2020 05:11:28 saved to disk.\n"],"name":"stderr"}]}]}